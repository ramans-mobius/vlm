name: Train Generic Image Captioning COMPATIBLE
description: Training with proper GIT model integration - FIXED
inputs:
  - name: tokenizer_json
    type: Model
  - name: vision_encoder_config
    type: Data
  - name: caption_dataset
    type: Dataset
  - name: training_parameters
    type: String
    default: "{}"
outputs:
  - name: trained_model
    type: Model
  - name: training_report
    type: Data
  - name: loss_curves
    type: Data
  - name: schema_json
    type: Data
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v29
    command:
      - sh
      - -c
      - |
        python3 -c "
        import sys, os, json, pickle, base64, io
        import subprocess
        
        print('=== SETTING UP EXACT VERSION MATCH ===')
        
        # Install EXACT versions from your local environment
        print('Installing exact versions from your local environment...')
        subprocess.call([sys.executable, '-m', 'pip', 'install', 'torch==2.1.0+cpu', '-f', 'https://download.pytorch.org/whl/torch_stable.html', '-q'])
        subprocess.call([sys.executable, '-m', 'pip', 'install', 'torchvision==0.16.0+cpu', '-f', 'https://download.pytorch.org/whl/torch_stable.html', '-q'])
        subprocess.call([sys.executable, '-m', 'pip', 'install', 'transformers==4.37.2', '-q'])
        subprocess.call([sys.executable, '-m', 'pip', 'install', 'tokenizers==0.15.2', '-q'])
        subprocess.call([sys.executable, '-m', 'pip', 'install', 'datasets==4.1.1', '-q'])
        subprocess.call([sys.executable, '-m', 'pip', 'install', 'pillow==11.3.0', '-q'])
        
        print('Version matching completed')
        
        # Now proceed with training
        import torch
        import torch.nn as nn
        import numpy as np
        from PIL import Image
        
        print('=== VERSION VERIFICATION ===')
        print(f'torch: {torch.__version__}')
        
        # Get arguments
        tokenizer_input = sys.argv[1]
        vision_config_input = sys.argv[2]
        dataset_input = sys.argv[3]
        training_params_input = sys.argv[4]
        model_output = sys.argv[5]
        report_output = sys.argv[6]
        loss_output = sys.argv[7]
        schema_output = sys.argv[8]
        
        print('=== INPUT VERIFICATION ===')
        print(f'tokenizer_input exists: {os.path.exists(tokenizer_input)}')
        print(f'vision_config_input exists: {os.path.exists(vision_config_input)}')
        print(f'dataset_input exists: {os.path.exists(dataset_input)}')
        
        # Parse training parameters
        training_params = {}
        try:
            if training_params_input.strip() and training_params_input.strip() != '{}':
                training_params = json.loads(training_params_input)
        except:
            pass
            
        epochs = training_params.get('epochs', 1)
        learning_rate = training_params.get('learning_rate', 5e-5)
        
        print(f'Training: {epochs} epochs, lr={learning_rate}')
        
        # Load vision config
        vision_config = {'model_name': 'openai/clip-vit-base-patch32', 'projection_dim': 512}
        try:
            if os.path.exists(vision_config_input):
                with open(vision_config_input, 'rb') as f:
                    vision_config = pickle.load(f)
        except:
            pass
        
        # Load dataset
        caption_data = []
        try:
            if os.path.exists(dataset_input):
                with open(dataset_input, 'rb') as f:
                    caption_data = pickle.load(f)
                print(f'Loaded {len(caption_data)} samples')
                # Debug: show sample data structure
                if caption_data:
                    print(f'Sample data keys: {list(caption_data[0].keys())}')
                    print(f'Sample caption: {caption_data[0].get(\\\"caption\\\", \\\"NO_CAPTION\\\")}')
        except Exception as e:
            print(f'Dataset load failed: {e}')
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f'Device: {device}')
        
        print('=== LOADING COMPONENTS ===')
        
        try:
            # Use nesy_factory components like in your working local code
            from nesy_factory.language_model.image_tokenizer import HFTokenizer
            from nesy_factory.language_model.vision_encoders import CLIPVisionEncoder
            
            print('Loading tokenizer...')
            # Load tokenizer from the provided path
            tokenizer = HFTokenizer()
            tokenizer_result = tokenizer.run(tokenizer_name='microsoft/git-base')
            print(f'Tokenizer loaded: {tokenizer_result}')
            
            print('Loading vision encoder...')
            vision_encoder = CLIPVisionEncoder(
                model_name=vision_config.get('model_name', 'openai/clip-vit-base-patch32'),
                projection_dim=vision_config.get('projection_dim', 512)
            )
            print('Vision encoder loaded')
            
            print('Loading GIT model...')
            from transformers import GitForCausalLM, GitProcessor
            
            # Load the actual GIT model that's designed for image captioning
            git_model = GitForCausalLM.from_pretrained('microsoft/git-base')
            git_processor = GitProcessor.from_pretrained('microsoft/git-base')
            
            git_model = git_model.to(device)
            print('GIT model loaded successfully')
            
            print('All components loaded successfully!')
            
        except Exception as e:
            print(f'Component loading failed: {e}')
            import traceback
            traceback.print_exc()
            # Fallback to transformers-only approach
            try:
                from transformers import AutoTokenizer, GitForCausalLM, GitProcessor
                
                print('Trying fallback loading...')
                tokenizer = AutoTokenizer.from_pretrained('microsoft/git-base')
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                git_model = GitForCausalLM.from_pretrained('microsoft/git-base')
                git_processor = GitProcessor.from_pretrained('microsoft/git-base')
                
                git_model = git_model.to(device)
                print('Fallback loading successful')
                
            except Exception as e2:
                print(f'Fallback also failed: {e2}')
                sys.exit(1)
        
        print('=== STARTING TRAINING ===')
        
        # Create output directories
        for path in [model_output, report_output, loss_output, schema_output]:
            os.makedirs(os.path.dirname(path), exist_ok=True)
        
        try:
            # Use only the GIT model parameters for training
            optimizer = torch.optim.AdamW(
                git_model.parameters(),
                lr=learning_rate
            )
            
            train_losses = []
            
            for epoch in range(epochs):
                epoch_loss = 0.0
                processed = 0
                successful_samples = 0
                
                for i, sample in enumerate(caption_data):
                    try:
                        optimizer.zero_grad()
                        
                        # Process image using GIT processor (not manual fusion)
                        try:
                            image_bytes = base64.b64decode(sample['image_data'])
                            image = Image.open(io.BytesIO(image_bytes))
                            if image.mode != 'RGB':
                                image = image.convert('RGB')
                            
                            # Get caption
                            caption = sample.get('caption', '')
                            if not caption:
                                print(f'Sample {i} has no caption, skipping')
                                continue
                            
                            # Process with GIT processor (handles both image and text)
                            inputs = git_processor(
                                images=image,
                                text=caption,
                                return_tensors='pt',
                                padding=True,
                                truncation=True,
                                max_length=128,
                                return_attention_mask=True
                            )
                            
                            # Move to device
                            inputs = {k: v.to(device) for k, v in inputs.items()}
                            
                        except Exception as e:
                            print(f'Image/text processing failed for sample {i}: {e}')
                            continue
                        
                        # Forward pass through GIT model (it handles the fusion internally)
                        outputs = git_model(
                            pixel_values=inputs.get('pixel_values'),
                            input_ids=inputs.get('input_ids'),
                            attention_mask=inputs.get('attention_mask'),
                            labels=inputs.get('input_ids')  # Use input_ids as labels for causal LM
                        )
                        
                        loss = outputs.loss
                        
                        # Only backward if loss is valid
                        if not torch.isnan(loss) and not torch.isinf(loss) and loss > 0:
                            loss.backward()
                            optimizer.step()
                            epoch_loss += loss.item()
                            processed += 1
                            successful_samples += 1
                            
                            if successful_samples % 10 == 0:
                                print(f'Epoch {epoch+1}, sample {successful_samples}, loss: {loss.item():.4f}')
                        else:
                            print(f'Sample {i} produced invalid loss: {loss}')
                            
                    except Exception as e:
                        if processed < 5:  # Only show first few errors in detail
                            print(f'Sample {i} failed: {e}')
                            import traceback
                            traceback.print_exc()
                        continue
                
                if successful_samples > 0:
                    avg_loss = epoch_loss / successful_samples
                    train_losses.append(avg_loss)
                    print(f'Epoch {epoch+1} completed: avg_loss={avg_loss:.4f}, successful_samples={successful_samples}/{processed}')
                else:
                    train_losses.append(0.0)
                    print(f'Epoch {epoch+1}: no successful samples processed')
            
            final_loss = train_losses[-1] if train_losses else 0.0
            
            # Save outputs
            torch.save({
                'git_model_state_dict': git_model.state_dict(),
                'training_params': training_params,
                'final_loss': final_loss,
                'epochs_trained': len(train_losses),
                'successful_samples': successful_samples,
                'model_type': 'git_image_captioning'
            }, model_output)
            
            report = {
                'status': 'success',
                'epochs_completed': len(train_losses),
                'final_loss': float(final_loss),
                'successful_samples': successful_samples,
                'total_samples': len(caption_data),
                'device_used': device,
                'model_used': 'microsoft/git-base',
                'training_approach': 'git_integrated',
                'versions': {
                    'torch': torch.__version__,
                    'transformers': '4.37.2'
                }
            }
            
            with open(report_output, 'w') as f:
                json.dump(report, f, indent=2)
            
            loss_data = {
                'epochs': list(range(1, len(train_losses) + 1)),
                'train_loss': [float(loss) for loss in train_losses],
                'successful_samples_per_epoch': [successful_samples] * len(train_losses)
            }
            
            with open(loss_output, 'w') as f:
                json.dump(loss_data, f, indent=2)
            
            schema = {
                'training_completed': True,
                'model_type': 'git_image_captioning',
                'final_loss': float(final_loss),
                'epochs_trained': len(train_losses),
                'successful_samples': successful_samples,
                'architecture': 'git_integrated'
            }
            
            with open(schema_output, 'w') as f:
                json.dump(schema, f, indent=2)
            
            print('=== TRAINING COMPLETED SUCCESSFULLY! ===')
            print(f'Final loss: {final_loss:.4f}')
            print(f'Successful samples: {successful_samples}/{len(caption_data)}')
            
        except Exception as e:
            print(f'=== TRAINING FAILED: {e} ===')
            import traceback
            traceback.print_exc()
            
            # Create minimal outputs
            for path in [model_output, report_output, loss_output, schema_output]:
                os.makedirs(os.path.dirname(path), exist_ok=True)
                if path.endswith('.json'):
                    with open(path, 'w') as f:
                        json.dump({'status': 'error', 'error': str(e)}, f)
                else:
                    torch.save({'status': 'error', 'error': str(e)}, path)
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputPath: tokenizer_json}
      - {inputPath: vision_encoder_config}
      - {inputPath: caption_dataset}
      - {inputValue: training_parameters}
      - {outputPath: trained_model}
      - {outputPath: training_report}
      - {outputPath: loss_curves}
      - {outputPath: schema_json}

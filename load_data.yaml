name: Load Data 
description: Universal data loader for image captioning - supports Hugging Face, CDN ZIP, and custom datasets.
inputs:
  - {name: data_source, type: String, description: 'Dataset source: huggingface://dataset-name, cdn://url, or custom://type'}
  - {name: data_config, type: String, default: "{}", description: 'JSON config for dataset-specific parameters'}
  - {name: max_samples, type: Integer, default: "0", description: 'Max samples to load (0 = all)'}
  - {name: caption_strategy, type: String, default: "auto", description: 'auto, use_existing, folder_based, filename_based'}
outputs:
  - {name: caption_dataset, type: Dataset}
  - {name: dataset_metadata, type: DatasetInfo}
  - {name: schema_json, type: Data}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v29
    command:
      - sh
      - -c
      - |
        python3 -c "
        import sys, os, pickle, json, base64, io, zipfile
        from urllib.parse import unquote, urlparse
        from PIL import Image
        import numpy as np
        import requests
        
        print('Number of arguments:', len(sys.argv))
        print('Arguments:', sys.argv)
        
        # Get args directly from command line
        data_source = sys.argv[1]
        data_config = sys.argv[2]
        max_samples = int(sys.argv[3])
        caption_strategy = sys.argv[4]
        caption_dataset_path = sys.argv[5]
        dataset_metadata_path = sys.argv[6]
        schema_output_path = sys.argv[7]
        
        print('Starting dataset loading...')
        print('Data source: ' + data_source)
        print('Data config: ' + data_config)
        print('Max samples: ' + str(max_samples))
        print('Caption strategy: ' + caption_strategy)
        
        # Parse data config
        try:
            config = json.loads(data_config) if data_config and data_config.strip() not in ['{}', ''] else {}
        except json.JSONDecodeError as e:
            print('WARN: Invalid data_config JSON: ' + str(e) + ', using empty config')
            config = {}

        def load_cdn_zip_dataset(url, config, max_samples, strategy):
            print('Loading CDN ZIP from: ' + url)
            
            # Download and extract
            decoded_url = unquote(url)
            headers = {'User-Agent': 'Mozilla/5.0'}
            
            try:
                response = requests.get(decoded_url, headers=headers, timeout=60)
                response.raise_for_status()
            except requests.RequestException as e:
                raise Exception('Failed to download from ' + url + ': ' + str(e))
            
            zip_content = io.BytesIO(response.content)
            caption_data = []
            
            try:
                with zipfile.ZipFile(zip_content, 'r') as zip_file:
                    # Get all image files
                    image_files = []
                    for file_path in zip_file.namelist():
                        lower_path = file_path.lower()
                        if any(lower_path.endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp']):
                            image_files.append(file_path)
                    
                    print('Found ' + str(len(image_files)) + ' image files in ZIP')
                    
                    if max_samples > 0:
                        image_files = image_files[:max_samples]
                        print('Limiting to ' + str(max_samples) + ' samples')
                    
                    for i, file_path in enumerate(image_files):
                        try:
                            with zip_file.open(file_path) as image_file:
                                image_data = image_file.read()
                                
                                # Verify it's a valid image
                                try:
                                    image = Image.open(io.BytesIO(image_data))
                                    image.verify()
                                except Exception as e:
                                    print('Skipping invalid image ' + file_path + ': ' + str(e))
                                    continue
                                
                                base64_image = base64.b64encode(image_data).decode('utf-8')
                                
                                # Generate caption based on strategy
                                caption = generate_caption_from_path(file_path, strategy)
                                
                                caption_data.append({
                                    'image_data': base64_image,
                                    'caption': caption,
                                    'original_caption': caption,
                                    'sample_id': i,
                                    'filename': file_path,
                                    'dataset_source': 'cdn:' + url,
                                    'caption_source': strategy
                                })
                                
                                if (i + 1) % 100 == 0:
                                    print('Processed ' + str(i + 1) + ' images...')
                                    
                        except Exception as e:
                            print('Skipping ' + file_path + ': ' + str(e))
                            continue
                            
            except zipfile.BadZipFile:
                raise Exception('Downloaded file is not a valid ZIP archive')
            
            return caption_data

        def generate_caption_from_path(file_path, strategy):
            filename = os.path.splitext(os.path.basename(file_path))[0]
            path_parts = [p for p in file_path.split('/') if p]
            
            # Use simple string concatenation instead of f-strings to avoid escaping issues
            if strategy == 'folder_based' and len(path_parts) > 1:
                class_name = path_parts[-2].replace('_', ' ')
                return 'A ' + class_name
            elif strategy == 'filename_based':
                clean_filename = filename.replace('_', ' ')
                return 'An image of ' + clean_filename
            else:  # auto or default
                if len(path_parts) > 1:
                    class_name = path_parts[-2].replace('_', ' ')
                    clean_filename = filename.replace('_', ' ')
                    return 'A ' + class_name + ' showing ' + clean_filename
                else:
                    clean_filename = filename.replace('_', ' ')
                    return 'An image of ' + clean_filename

        # Main loading logic
        if data_source.startswith('cdn://'):
            actual_url = data_source.replace('cdn://', '')
            dataset = load_cdn_zip_dataset(actual_url, config, max_samples, caption_strategy)
            
            metadata = {
                'source_type': 'cdn_zip',
                'cdn_url': actual_url,
                'total_samples': len(dataset),
                'caption_strategy': caption_strategy,
                'data_quality': 'medium',
                'config_used': config
            }
            
        else:
            raise ValueError('Unsupported data source protocol: ' + data_source)

        # Ensure output directories exist
        os.makedirs(os.path.dirname(caption_dataset_path) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(dataset_metadata_path) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(schema_output_path) or '.', exist_ok=True)

        # Save outputs
        with open(caption_dataset_path, 'wb') as f:
            pickle.dump(dataset, f)

        with open(dataset_metadata_path, 'wb') as f:
            pickle.dump(metadata, f)

        schema = {
            'data_source': data_source,
            'loading_strategy': caption_strategy,
            'dataset_info': metadata,
            'universal_compatibility': True,
            'samples_loaded': len(dataset)
        }
        
        with open(schema_output_path, 'w') as f:
            json.dump(schema, f, indent=2)

        print('Dataset loading complete! Loaded ' + str(len(dataset)) + ' samples from ' + data_source)
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6"
    args:
      - {inputValue: data_source}
      - {inputValue: data_config}
      - {inputValue: max_samples}
      - {inputValue: caption_strategy}
      - {outputPath: caption_dataset}
      - {outputPath: dataset_metadata}
      - {outputPath: schema_json}

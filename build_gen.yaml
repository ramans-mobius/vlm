name: Gen Build Image Captioning Model
description: Builds image captioning models from JSON blueprint
inputs:
  - name: tokenizer_json
    type: Model
  - name: model_blueprint
    type: String
    default: "{}"
  - name: domain_specialization
    type: String
    default: "general"
outputs:
  - name: model_config
    type: Data
  - name: model_weights
    type: Model
  - name: model_py
    type: Data
  - name: schema_json
    type: Data
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v29
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import sys, os, json, torch

        # Parse arguments
        tokenizer_input = sys.argv[1]
        model_blueprint_input = sys.argv[2]
        domain_specialization = sys.argv[3]
        model_config_path = sys.argv[4]
        model_weights_path = sys.argv[5]
        model_py_path = sys.argv[6]
        schema_output_path = sys.argv[7]

        print('Building image captioning model')
        print(f'RAW model_blueprint_input received: "{model_blueprint_input}"')

        # Parse model blueprint - SIMPLE DIRECT APPROACH
        blueprint = {}
        try:
            if model_blueprint_input.strip() and model_blueprint_input.strip() != '{}':
                blueprint = json.loads(model_blueprint_input)
                print('✓ Blueprint JSON parse successful')
            else:
                blueprint = {'architecture': 'git', 'model_type': 'image_captioning'}
                print('⚠ Using default blueprint')
        except Exception as e:
            print(f'Blueprint parse failed: {e}')
            blueprint = {'architecture': 'git', 'model_type': 'image_captioning'}
            print('⚠ Using default blueprint due to parse error')

        print(f'Final blueprint: {blueprint}')

        architecture = blueprint.get('architecture', 'git')
        print(f'Architecture: {architecture}')

        model_config = {
            'architecture': architecture,
            'model_type': blueprint.get('model_type', 'image_captioning'),
            'domain_specialization': domain_specialization,
            'blueprint': blueprint,
            'max_caption_length': blueprint.get('max_caption_length', 128),
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }

        if architecture == 'git':
            model_code = '''import torch
from transformers import GitForCausalLM, GitProcessor

class GITImageCaptioningModel:
    def __init__(self, config):
        self.config = config
        self.model = GitForCausalLM.from_pretrained(config['blueprint'].get('language_model', 'microsoft/git-base'))
        self.processor = GitProcessor.from_pretrained(config['blueprint'].get('language_model', 'microsoft/git-base'))
        
    def to(self, device):
        self.model = self.model.to(device)
        return self
        
    def train(self):
        self.model.train()
        
    def eval(self):
        self.model.eval()
        
    def forward(self, images, input_ids=None, attention_mask=None, labels=None):
        inputs = self.processor(images=images, text=None if input_ids is None else [''], return_tensors='pt', padding=True, truncation=True, max_length=self.config['max_caption_length'])
        if input_ids is not None:
            inputs['input_ids'] = input_ids
            inputs['attention_mask'] = attention_mask
        return self.model(**inputs, labels=labels)
        
    def generate(self, images, max_length=128, temperature=1.0):
        self.eval()
        inputs = self.processor(images=images, return_tensors='pt')
        return self.model.generate(**inputs, max_length=max_length, temperature=temperature, do_sample=True)
'''
        else:
            model_code = '''import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, CLIPModel, CLIPProcessor

class CLIPLLMCaptioningModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        blueprint = config['blueprint']
        components = blueprint.get('components', {})
        
        vision_cfg = components.get('vision_encoder', {})
        self.clip_model = CLIPModel.from_pretrained(vision_cfg.get('model_name', 'openai/clip-vit-base-patch32'))
        self.clip_processor = CLIPProcessor.from_pretrained(vision_cfg.get('model_name', 'openai/clip-vit-base-patch32'))
        
        text_cfg = components.get('text_decoder', {})
        self.language_model = AutoModelForCausalLM.from_pretrained(text_cfg.get('model_name', 'gpt2'))
        self.tokenizer = AutoTokenizer.from_pretrained(text_cfg.get('model_name', 'gpt2'))
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        clip_dim = self.clip_model.config.projection_dim
        text_dim = self.language_model.config.hidden_size
        
        if clip_dim != text_dim:
            self.vision_proj = nn.Linear(clip_dim, text_dim)
        else:
            self.vision_proj = nn.Identity()
        
        if vision_cfg.get('unfreeze_layers', 0) == 0:
            for param in self.clip_model.parameters():
                param.requires_grad = False

    def forward(self, images, input_ids=None, attention_mask=None, labels=None):
        clip_inputs = self.clip_processor(images=images, return_tensors='pt')
        device = next(self.language_model.parameters()).device
        clip_inputs = {k: v.to(device) for k, v in clip_inputs.items()}
        
        with torch.no_grad():
            clip_features = self.clip_model.get_image_features(**clip_inputs)
        
        visual_features = self.vision_proj(clip_features)
        
        if input_ids is not None:
            text_embeddings = self.language_model.get_input_embeddings()(input_ids)
            fused_embeddings = text_embeddings.clone()
            fused_embeddings[:, 0:1, :] += visual_features.unsqueeze(1)
            
            outputs = self.language_model(inputs_embeds=fused_embeddings, attention_mask=attention_mask, labels=labels)
            return outputs
        else:
            return visual_features

    def generate(self, images, max_length=128, temperature=1.0):
        self.eval()
        with torch.no_grad():
            clip_inputs = self.clip_processor(images=images, return_tensors='pt')
            device = next(self.language_model.parameters()).device
            clip_inputs = {k: v.to(device) for k, v in clip_inputs.items()}
            clip_features = self.clip_model.get_image_features(**clip_inputs)
            visual_features = self.vision_proj(clip_features)
            
            batch_size = visual_features.shape[0]
            input_ids = torch.tensor([[self.tokenizer.bos_token_id]] * batch_size).to(device)
            
            for _ in range(max_length):
                text_embeddings = self.language_model.get_input_embeddings()(input_ids)
                
                if input_ids.shape[1] == 1:
                    fused_embeddings = text_embeddings + visual_features.unsqueeze(1)
                else:
                    fused_embeddings = text_embeddings
                
                outputs = self.language_model(inputs_embeds=fused_embeddings)
                next_token_logits = outputs.logits[:, -1, :] / temperature
                next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                input_ids = torch.cat([input_ids, next_tokens], dim=1)
                
                if (next_tokens == self.tokenizer.eos_token_id).all():
                    break
            
            return input_ids
'''

        # Create output directories and save files
        os.makedirs(os.path.dirname(model_config_path), exist_ok=True)
        with open(model_config_path, 'w') as f:
            json.dump(model_config, f, indent=2)

        os.makedirs(os.path.dirname(model_py_path), exist_ok=True)
        with open(model_py_path, 'w') as f:
            f.write(model_code)

        initial_state = {
            'architecture': architecture,
            'model_type': model_config['model_type'],
            'config': model_config,
            'blueprint': blueprint,
            'status': 'initialized'
        }
        
        os.makedirs(os.path.dirname(model_weights_path), exist_ok=True)
        torch.save(initial_state, model_weights_path)

        schema = {
            'model_type': model_config['model_type'],
            'architecture': architecture,
            'status': 'built'
        }
        
        os.makedirs(os.path.dirname(schema_output_path), exist_ok=True)
        with open(schema_output_path, 'w') as f:
            json.dump(schema, f, indent=2)

        print('Model built successfully')
    args:
      - {inputPath: tokenizer_json}
      - {inputValue: model_blueprint}
      - {inputValue: domain_specialization}
      - {outputPath: model_config}
      - {outputPath: model_weights}
      - {outputPath: model_py}
      - {outputPath: schema_json}
